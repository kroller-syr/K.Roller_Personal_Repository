---
title: "Pixel Data Analysis Using PCA and Naive Bayes"
author: "Kent Roller"
date: "2024-08-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading in libraries that will be required to perform the task assigned 
```{r}
library(e1071)
library(naivebayes)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
```
All packages loaded without incident 

Next, need to load the data, to do this, will first use file.choose to select each file location and then comment the file.choose out so it does not run during knitting or subsequent loads. 

```{r}
#file.choose()
```

```{r}
data_train<-read.csv("C:\\Users\\super\\Downloads\\digit_train.csv", stringsAsFactors = TRUE, header=TRUE)
```

Training data loaded in, now need to load in the testing data
```{r}
file.choose()
```

```{r}
data_test<-read.csv("C:\\Users\\super\\Downloads\\digit_test.csv", stringsAsFactors = TRUE)
```

OK, data sets are loaded in, now need to proceed to next step
Need to convert label to nominal variables in the data, for both datasets. 

```{r}
data_train$label<-as.factor(data_train$label)
#test data does not appear to have a variable named label 
dim(data_train)
head(data_train)
```

Need to reduce dimensionality but dont want to do it randomly, 
instead, want to use a method that can help determine best feature 
selection for model, using PCA this decision can be made based on the variance between features, where principal components represent the the directions of data the explain the maximal amount of variance. The larger the variance carried by a line, the larger the dispersion of data points along that line, and the bigger that dispersion is the more information that line has. To perform this will use FactoMineR. 

```{r}
library(FactoMineR)
pca_select1=PCA(t(select(data_train, -label)))
data_train1=data.frame(data_train$label, pca_select1$var$coord)
```

Looking at the output in the graphs, we can see that the PCA found the first linear combination of variables covered 30.89% of the varaiance found within the data, and the second linear combination of variables. Should be able to view the percentage contribution of each variable by viewing the eigen values which are created when using PCA. But, will need to call them specifically. 

```{r}
vari_data_train1<- pca_select1$eig[1:6,2]
barplot(vari_data_train1, main="Variance Explained by PCA", 
        xlab="Principal Components", 
        ylab="Percentage of Variance Explained", 
        names.arg=paste0("PC", 1:length(vari_data_train1)))
```
Here we can see a reflection of the findings in the graph, where the first dimension explains over 30% of the variance in the data, and then second dimension is just over 5. To get a better idea of how this decision is made we can look at a scree plot to view the point of inflection in the PCA. 

```{r}
eigens_data_train1<-pca_select1$eig[1:10,1]
plot(eigens_data_train1, 
     type="b", 
     main="Scree Plot", 
     xlab="Principal Components", 
     ylab="Eigenvalues", 
     pch=18, 
     col="skyblue")
```


Likewise, can modify PCA with arguments to change the nummber of dimensions kept, will produce for additional testing in accuracy. 

```{r}
pca_select2=PCA(t(select(data_train, -label)), ncp=10)
data_train2=data.frame(data_train$label, pca_select2$var$coord)
```


```{r}
pca_select3=PCA(t(select(data_train, -label)), ncp=20)
data_train3=data.frame(data_train$label, pca_select3$var$coord)
```


```{r}
pca_select4=PCA(t(select(data_train, -label)), ncp=2)
data_train4=data.frame(data_train$label, pca_select4$var$coord)
```
OK, now we have produced 4 different cuts to the data based on different dimensionality reduction select points. The original was automatically selected by the PCA algorithim, where as the 3 that followed were generated by using the ncp agument in the PCA function and manually setting the number of dimensions to keep. The additional cuts are for testing of accuracy later on. 


However, this still leaves us with 42000 observations which may not be required. We can next reduce each of the created training sets to acheive this. 

```{r}
percent<-.25
set.seed(42)
data_train1_split<-sample(nrow(data_train1), nrow(data_train1)*percent)
data_train1_splitted<-data_train1[data_train1_split,]
dim(data_train1_splitted)
```

Ok, we now have a quarter of the orginal observations, so we have went from a 33 million object to work with to a 60,500 object to work with. A fairly drastic reduction in the overall dimensions of the dataset. Now need to do the same with the other datasets generated for additional testing and comparison later on. 

```{r}
data_train2_split<-sample(nrow(data_train2), nrow(data_train2)*percent)
data_train2_splitted<-data_train2[data_train2_split,]
dim(data_train2_splitted)
```


```{r}
data_train3_split<-sample(nrow(data_train3), nrow(data_train3)*percent)
data_train3_splitted<-data_train3[data_train3_split,]
dim(data_train3_splitted)
```



```{r}
data_train4_split<-sample(nrow(data_train4), nrow(data_train4)*percent)
data_train4_splitted<-data_train4[data_train4_split,]
dim(data_train4_splitted)
```

Now perform cross validation using K-fold cross validation 
```{r}
N<-nrow(data_train1_splitted)
kfolds<-10
holdout<-split(sample(1:N), 1:kfolds)
```

```{r}
N1<-nrow(data_train2_splitted)
kfolds1<-10
holdout1<-split(sample(1:N1), 1:kfolds)
```

```{r}
N2<-nrow(data_train3_splitted)
holdout2<-split(sample(1:N2), 1:kfolds)
```


```{r}
N3<-nrow(data_train4_splitted)
holdout3<-split(sample(1:N3), 1:kfolds)
```




Cross Validation Results

```{r}
#Run training and Testing for each of the k-folds
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train1_splitted[holdout[[k]], ]
    data_cross_train <- data_train1_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label

    # Naive Bayes Train model
    train_naibayes <- naiveBayes(data_train.label ~ ., data = data_cross_train, na.action = na.pass)
    
    # Naive Bayes model Prediction 
    nb_Pred <- predict(train_naibayes, data_cross_test_noLabel)
    
    # Ensure the predicted values are factors with the same levels
    nb_Pred <- factor(nb_Pred, levels = levels(data_cross_train$data_train.label))

    # Testing accuracy of Naive Bayes model
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))
    
    # Accumulate results from each fold
    AllResults <- c(AllResults, nb_Pred)
    AllLabels <- c(AllLabels, data_cross_test_justLabel)
}

# Visualize
plot(factor(AllResults, levels = levels(data_train1$data_train.label)), ylab = "Density", main = "Naive Bayes Plot")
```


Now visulaize the results

```{r}
(table(unlist(AllResults), unlist(AllLabels)))
```


```{r}
accuracy_results <- numeric(kfolds)
    # Calculate the accuracy
    accuracy <- sum(nb_Pred == data_cross_test_justLabel) / length(nb_Pred)
    
    # Store the accuracy for this fold
    accuracy_results[k] <- accuracy
    
    # Print the confusion matrix for this fold (optional)
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))


# Output the accuracy for each fold
print(accuracy_results)

# Calculate and output the overall average accuracy
average_accuracy <- mean(accuracy_results)
cat("Average accuracy across all folds:", average_accuracy, "\n")
```




Now to run it again but this time with a manual selection of the dimensions to test accuracy across data set sizes

For the following, this will be using 11 variables instead of the 6 initially selected via PCA
```{r}
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train2_splitted[holdout[[k]], ]
    data_cross_train <- data_train2_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label

    # Naive Bayes Train model
    train_naibayes <- naiveBayes(data_train.label ~ ., data = data_cross_train, na.action = na.pass)
    
    # Naive Bayes model Prediction 
    nb_Pred <- predict(train_naibayes, data_cross_test_noLabel)
    
    # Ensure the predicted values are factors with the same levels
    nb_Pred <- factor(nb_Pred, levels = levels(data_cross_train$data_train.label))

    # Testing accuracy of Naive Bayes model
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))
    
    # Accumulate results from each fold
    AllResults <- c(AllResults, nb_Pred)
    AllLabels <- c(AllLabels, data_cross_test_justLabel)
}

# Visualize
plot(factor(AllResults, levels = levels(data_train2$data_train.label)), ylab = "Density", main = "Naive Bayes Plot")
```



```{r}
accuracy_results <- numeric(kfolds)
    # Calculate the accuracy
    accuracy <- sum(nb_Pred == data_cross_test_justLabel) / length(nb_Pred)
    
    # Store the accuracy for this fold
    accuracy_results[k] <- accuracy
    
    # Print the confusion matrix for this fold (optional)
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))


# Output the accuracy for each fold
print(accuracy_results)

# Calculate and output the overall average accuracy
average_accuracy <- mean(accuracy_results)
cat("Average accuracy across all folds:", average_accuracy, "\n")
```
Using a larger number of dimensions seemed to help increase the accuracy of the model.

For the next test we will be using 21 variables instead of the 11 manually selected to see if we can get an additional increase in the accuracy if the prediction. 

```{r}
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train3_splitted[holdout[[k]], ]
    data_cross_train <- data_train3_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label

    # Naive Bayes Train model
    train_naibayes <- naiveBayes(data_train.label ~ ., data = data_cross_train, na.action = na.pass)
    
    # Naive Bayes model Prediction 
    nb_Pred <- predict(train_naibayes, data_cross_test_noLabel)
    
    # Ensure the predicted values are factors with the same levels
    nb_Pred <- factor(nb_Pred, levels = levels(data_cross_train$data_train.label))

    # Testing accuracy of Naive Bayes model
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))
    
    # Accumulate results from each fold
    AllResults <- c(AllResults, nb_Pred)
    AllLabels <- c(AllLabels, data_cross_test_justLabel)
}

# Visualize
plot(factor(AllResults, levels = levels(data_train3$data_train.label)), ylab = "Density", main = "Naive Bayes Plot")
```


```{r}
accuracy_results <- numeric(kfolds)
    # Calculate the accuracy
    accuracy <- sum(nb_Pred == data_cross_test_justLabel) / length(nb_Pred)
    
    # Store the accuracy for this fold
    accuracy_results[k] <- accuracy
    
    # Print the confusion matrix for this fold (optional)
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))


# Output the accuracy for each fold
print(accuracy_results)

# Calculate and output the overall average accuracy
average_accuracy <- mean(accuracy_results)
cat("Average accuracy across all folds:", average_accuracy, "\n")
```



And finally we will perform one last iteration of the test where the dimensions have been reduced down to 3 variables based off of the inflection point of the eigen value graph displayed previously. 

```{r}
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train4_splitted[holdout[[k]], ]
    data_cross_train <- data_train4_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label

    # Naive Bayes Train model
    train_naibayes <- naiveBayes(data_train.label ~ ., data = data_cross_train, na.action = na.pass)
    
    # Naive Bayes model Prediction 
    nb_Pred <- predict(train_naibayes, data_cross_test_noLabel)
    
    # Ensure the predicted values are factors with the same levels
    nb_Pred <- factor(nb_Pred, levels = levels(data_cross_train$data_train.label))

    # Testing accuracy of Naive Bayes model
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))
    
    # Accumulate results from each fold
    AllResults <- c(AllResults, nb_Pred)
    AllLabels <- c(AllLabels, data_cross_test_justLabel)
}

# Visualize
plot(factor(AllResults, levels = levels(data_train4$data_train.label)), ylab = "Density", main = "Naive Bayes Plot")
```


```{r}
accuracy_results <- numeric(kfolds)
    # Calculate the accuracy
    accuracy <- sum(nb_Pred == data_cross_test_justLabel) / length(nb_Pred)
    
    # Store the accuracy for this fold
    accuracy_results[k] <- accuracy
    
    # Print the confusion matrix for this fold (optional)
    print(confusionMatrix(nb_Pred, data_cross_test_justLabel))


# Output the accuracy for each fold
print(accuracy_results)

# Calculate and output the overall average accuracy
average_accuracy <- mean(accuracy_results)
cat("Average accuracy across all folds:", average_accuracy, "\n")
```

And this concludes the section dedicated to naive bayes. For the next section we will instead use decision trees to see how it compares to the naive bayes method. 

To do this we can use the same kfolds and PCA splits obtained from earlier, and plug these into the decision tree directly. 
However, since we did not rename variables between runs, will need to re-run the code for each dimension of the training data.

Specifically, we will recycle this section of code, but instead of initializing a naive bayes model we will instead proceed with a decision tree model. 
```{r}

accuracy_results_decision_tree <- numeric(kfolds)
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train1_splitted[holdout[[k]], ]
    data_cross_train <- data_train1_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label
    
     ### Decision Tree Model
    train_tree <- rpart(data_train.label ~ ., data = data_cross_train, method = "class")
    dt_Pred <- predict(train_tree, data_cross_test_noLabel, type = "class")
    dt_Pred <- factor(dt_Pred, levels =     levels(data_cross_train$data_train.label))
    accuracy_decision_tree <- sum(dt_Pred == data_cross_test_justLabel) / length(dt_Pred)
    accuracy_results_decision_tree[k] <- accuracy_decision_tree
    
    
    cat("Fold", k, "- Decision Tree Confusion Matrix:\n")
    print(confusionMatrix(dt_Pred, data_cross_test_justLabel))
    
}
```


Once again we now perform the same test but this time on the dataset that was split to contain 11 variables instead of 6. 
```{r}
accuracy_results_decision_tree <- numeric(kfolds)
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train2_splitted[holdout[[k]], ]
    data_cross_train <- data_train2_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label
    
     ### Decision Tree Model
    train_tree <- rpart(data_train.label ~ ., data = data_cross_train, method = "class")
    dt_Pred <- predict(train_tree, data_cross_test_noLabel, type = "class")
    dt_Pred <- factor(dt_Pred, levels =     levels(data_cross_train$data_train.label))
    accuracy_decision_tree <- sum(dt_Pred == data_cross_test_justLabel) / length(dt_Pred)
    accuracy_results_decision_tree[k] <- accuracy_decision_tree
    
    
    cat("Fold", k, "- Decision Tree Confusion Matrix:\n")
    print(confusionMatrix(dt_Pred, data_cross_test_justLabel))
    
}
```
```{r}
# Plot the decision tree
rpart.plot(
  train_tree,
  type = 2,
  extra = 104,
  fallen.leaves = TRUE,
  main = "Decision Tree",
  cex = 0.6,
  under = TRUE,
  box.palette = "RdYlGn",
  shadow.col = "gray"
)
```



```{r}
accuracy_results_decision_tree <- numeric(kfolds)
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train3_splitted[holdout[[k]], ]
    data_cross_train <- data_train3_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label
    
     ### Decision Tree Model
    train_tree <- rpart(data_train.label ~ ., data = data_cross_train, method = "class")
    dt_Pred <- predict(train_tree, data_cross_test_noLabel, type = "class")
    dt_Pred <- factor(dt_Pred, levels =     levels(data_cross_train$data_train.label))
    accuracy_decision_tree <- sum(dt_Pred == data_cross_test_justLabel) / length(dt_Pred)
    accuracy_results_decision_tree[k] <- accuracy_decision_tree
    
    
    cat("Fold", k, "- Decision Tree Confusion Matrix:\n")
    print(confusionMatrix(dt_Pred, data_cross_test_justLabel))
    
}
```
```{r}
# Plot the decision tree
rpart.plot(
  train_tree,
  type = 2,
  extra = 104,
  fallen.leaves = TRUE,
  main = "Decision Tree",
  cex = 0.6,
  under = TRUE,
  box.palette = "RdYlGn",
  shadow.col = "gray"
)
```




```{r}
accuracy_results_decision_tree <- numeric(kfolds)
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds) {
    data_cross_test <- data_train4_splitted[holdout[[k]], ]
    data_cross_train <- data_train4_splitted[-holdout[[k]], ]
    
    # Ensure the factor levels are consistent
    data_cross_test$data_train.label <- factor(data_cross_test$data_train.label, levels = levels(data_cross_train$data_train.label))
    
    # Create test data without labels
    data_cross_test_noLabel <- data_cross_test[-c(1)]
    data_cross_test_justLabel <- data_cross_test$data_train.label
    
     ### Decision Tree Model
    train_tree <- rpart(data_train.label ~ ., data = data_cross_train, method = "class")
    dt_Pred <- predict(train_tree, data_cross_test_noLabel, type = "class")
    dt_Pred <- factor(dt_Pred, levels =     levels(data_cross_train$data_train.label))
    accuracy_decision_tree <- sum(dt_Pred == data_cross_test_justLabel) / length(dt_Pred)
    accuracy_results_decision_tree[k] <- accuracy_decision_tree
    
    
    cat("Fold", k, "- Decision Tree Confusion Matrix:\n")
    print(confusionMatrix(dt_Pred, data_cross_test_justLabel))
    
}
```




```{r}
# Plot the decision tree
rpart.plot(
  train_tree,
  type = 2,
  extra = 104,
  fallen.leaves = TRUE,
  main = "Decision Tree",
  cex = 0.6,
  under = TRUE,
  box.palette = "RdYlGn",
  shadow.col = "gray"
)
```



































